{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df3727e4",
      "metadata": {},
      "source": [
        "# Assignment: Foundations of Bayesâ€™ theorem\n",
        "\n",
        "Fill out the blanks as per the instructions below.\n",
        "\n",
        "This assignment uses type hints, so make sure to stick to those.\n",
        "\n",
        "Whenever you need to fill in a blank, we used Python's ellipsis (`...`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7addde5",
      "metadata": {},
      "source": [
        "# Part 1 (A): Bayes' theorem with discrete random variables\n",
        "\n",
        "Here, we assume a discrete prior $P(\\theta)$, as well as a discrete probability distribution over a few i.i.d. observations.\n",
        "\n",
        "The goal is to manually implement functionals for computing marginal and conditional likelihoods/probability densities.\n",
        "\n",
        "You need to show that the posterior probability $P(\\theta|Y)$ is a proper probability mass function.\n",
        "Choose a different number of parameters and observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "01e7ecda",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's define our parameters theta and their probabilities (our prior belief):\n",
        "# A handful of thetas is enough.\n",
        "theta: list[int] = [1, 2, 3, 4, 5]\n",
        "theta_probs: list[float] = [0.1, 0.2, 0.4, 0.2, 0.1]\n",
        "\n",
        "# Here are our observations Y (don't change!):\n",
        "Y_obs = [0.5, 1.2]\n",
        "\n",
        "# Instead of assuming some (parameterized) distribution,\n",
        "# we hardcode the conditional likelihoods of Y given some theta.\n",
        "# Note that P(Y|\\theta) is a likelihood, so it does not represent\n",
        "# (necessarily) a valid probability density (i.e., values for each\n",
        "# \\theta do not necessarily have to sum to 1).\n",
        "P_Y_given_theta: dict[int, dict[float, float]] = {\n",
        "    1: {0.5: 0.8, 1.2: 0.3},\n",
        "    2: {0.5: 0.6, 1.2: 0.5},\n",
        "    3: {0.5: 0.4, 1.2: 0.8},\n",
        "    4: {0.5: 0.2, 1.2: 0.9},\n",
        "    5: {0.5: 0.1, 1.2: 0.7}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2bfcd27",
      "metadata": {},
      "source": [
        "## Define PMFs\n",
        "\n",
        "For convenience, we define the PMFs for $\\theta$ and $Y$ explicitly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d9e04636",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Don't change!\n",
        "def P_theta(val: float) -> float:\n",
        "    assert val in theta\n",
        "    idx = theta.index(val)\n",
        "    return theta_probs[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019a8211",
      "metadata": {},
      "source": [
        "## Define Functions\n",
        "\n",
        "for the likelihood $P(Y|\\theta)$, the prior $P(\\theta)$, and the evidence $P(Y)$.\n",
        "\n",
        "Recall that the evidence:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    P(Y)&=\\sum_i\\,P(Y|\\theta_i)\\times P(\\theta_i)\\nonumber.\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3e4c26f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Likelihood, P(Y|\\theta), now explicitly from our discrete definition:\n",
        "def likelihood(Y: list[float], t: float) -> float:\n",
        "\n",
        "    prob = 1.0\n",
        "    for y in Y:\n",
        "        if y in P_Y_given_theta[t]:\n",
        "            prob *= P_Y_given_theta[t][y]\n",
        "        else:\n",
        "            return 0.0  # If y is not defined for this theta, likelihood is zero.\n",
        "    return prob\n",
        "\n",
        "# The Evidence (in this assignment, it *is* computable):\n",
        "def P_Y(Y: list[float]) -> float:\n",
        "    evidence = 0.0\n",
        "    for t in theta:\n",
        "        evidence += P_theta(t) * likelihood(Y, t)\n",
        "    return evidence\n",
        "\n",
        "# The posterior:\n",
        "def P_theta_given_Y(t: float, Y: list[float]) -> float:\n",
        "    return (likelihood(Y, t) * P_theta(t)) / P_Y(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32560e80",
      "metadata": {},
      "source": [
        "# Part 1(B): Bayes' theorem with continuous random variables\n",
        "\n",
        "-------------------------\n",
        "\n",
        "Now, we change our model a bit.\n",
        "Instead of assuming a small discrete set of possible values for $\\theta$, we will assume that this parameter follows a standard normal distribution.\n",
        "\n",
        "For our actual model, we will assume another normal distribution, where the standard deviation (scale) is fixed at $\\frac{3}{2}$ and the mean is set to $\\theta$: $N\\sim(\\mu=\\theta,\\sigma=\\frac{3}{2})$.\n",
        "\n",
        "We will re-use the previous observations.\n",
        "\n",
        "\n",
        "The evidence, defined continuously:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    P(Y)=\\int_{\\theta}\\,P(Y|t)\\times P(t)\\,d\\theta\\nonumber.\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1da3f133",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.09412, 0.23529, 0.50196, 0.14118, 0.02745]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Don't change!\n",
        "posterior_probs = [round(P_theta_given_Y(t=t, Y=Y_obs), ndigits=5) for t in theta]\n",
        "posterior_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3a3298e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# Don't change! The result here needs to be ~1.0!\n",
        "print(sum(posterior_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8b3c74d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats.distributions import norm\n",
        "\n",
        "# Our prior:\n",
        "def P_theta_continuous(val: float) -> float:\n",
        "    # Use norm.pdf() to compute this.\n",
        "    return norm.pdf(val, loc=0, scale=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "82180d52",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.integrate import quad\n",
        "from typing import final\n",
        "\n",
        "\n",
        "# Realistically, our bounds could be -10,10 (or similar), but\n",
        "# scipy's quad allows to use infinity, so we'll use that, as\n",
        "# it's also closer to how we would formulate this mathematically.\n",
        "a, b = -np.inf, np.inf\n",
        "\n",
        "\n",
        "# Our model prototype that takes a single scale parameter that\n",
        "# will be held fixed for any subsequent likelihood computations.\n",
        "@final\n",
        "class Model:\n",
        "    \"\"\"Keep using this model class as-is, no need to change it.\"\"\"\n",
        "    def __init__(self, scale: float):\n",
        "        self.scale = scale\n",
        "    \n",
        "    def likelihood(self, x: float, mean: float) -> float:\n",
        "        return norm.pdf(x=x, loc=mean, scale=self.scale).item()\n",
        "    \n",
        "\n",
        "def likelihood_continuous(Y: list[float], t: float, model: Model) -> float:\n",
        "    prob = 1.0\n",
        "    for y in Y:\n",
        "        prob *= norm.pdf(x=y, loc=t, scale=model.scale)\n",
        "    return prob\n",
        "\n",
        "def P_Y_continuous(Y: list[float], model: Model) -> float:\n",
        "    \"\"\"Use quad() to integrate.\"\"\"\n",
        "    from scipy.integrate import quad\n",
        "    integrand = lambda t: likelihood_continuous(Y=Y, t=t, model=model) * P_theta_continuous(t)\n",
        "    evidence, _ = quad(func=integrand, a=a, b=b)\n",
        "    return evidence\n",
        "\n",
        "\n",
        "def P_theta_given_Y_continuous(t: float, Y: list[float], evidence: float, model: Model) -> float:\n",
        "    numerator = likelihood_continuous(Y=Y, t=t, model=model) * P_theta_continuous(t)\n",
        "    return numerator / evidence\n",
        "\n",
        "\n",
        "# The goal of this function is to assert that our posterior is\n",
        "# a valid probability density that sums/integrates to 1.\n",
        "def P_theta_given_Y_continuous_integral(Y: list[float], model: Model) -> float:\n",
        "    evidence = P_Y_continuous(Y=Y, model=model)\n",
        "    func = lambda t: P_theta_given_Y_continuous(Y=Y, t=t, model=model, evidence=evidence)\n",
        "    return quad(func=func, a=a, b=b)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a73924",
      "metadata": {},
      "source": [
        "Now we show the amount of evidence, as well as that our posterior integrates to $\\approx1$:\n",
        "Also, we show the amount of (log-)evidence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b8c34f27",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-3.1912461104301157, 0.999999999999999)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Don't change. Prints the log-evidence, as well as its integral (should be ~1.0).\n",
        "from math import log\n",
        "use_model = Model(scale=1.5)\n",
        "\n",
        "log(P_Y_continuous(Y=Y_obs, model=use_model)),\\\n",
        "P_theta_given_Y_continuous_integral(Y=Y_obs, model=use_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63cd0b42",
      "metadata": {},
      "source": [
        "### Find and use a better model\n",
        "\n",
        "Recall that our observations were fixed at $[0.5, 1.2]$ and we assumed our model would be a normal distribution with standard deviation $\\sigma=\\frac{3}{2}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "86018ecc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.35, 0.85)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_obs_arr = np.array(Y_obs)\n",
        "Y_obs_arr.std().item(), Y_obs_arr.mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb28220",
      "metadata": {},
      "source": [
        "However, we know that the standard deviation should likely be smaller to accommodate our data better.\n",
        "What we want to show here, is that a better model (here: same as previous but with a fixed standard deviation closer to $0.35$) produces a larger **evidence**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1f13fb8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize_scalar\n",
        "\n",
        "# Use 'minimize_scalar' to find some optimal solution\n",
        "optimal_scale = minimize_scalar(\n",
        "    lambda s: -P_Y_continuous(Y=Y_obs, model=Model(scale=s)),\n",
        "    bounds=(0.01, 1),\n",
        "    method='bounded'\n",
        ").x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "31db941e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-2.360448901823369, 1.0)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Don't change! Prints the log-evidence, as well as its integral (should be ~1.0).\n",
        "better_model = Model(scale=optimal_scale)\n",
        "\n",
        "log(P_Y_continuous(Y=Y_obs, model=better_model)),\\\n",
        "P_theta_given_Y_continuous_integral(Y=Y_obs, model=better_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c891f1",
      "metadata": {},
      "source": [
        "# Short evaluation (write 1-2 sentences per):\n",
        "\n",
        "1. How has the (log-)evidence changed using the optimal scale?\n",
        "2. In Bayesian terms, what does this result mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1073b2ed",
      "metadata": {},
      "source": [
        "**Answers**:\n",
        "\n",
        "1. The log-evidence has increased significantly from approximately -3.19 (with the original scale=1.5) to approximately -2.36 (with the optimal scale). This represents an improvement of about 0.83, meaning the evidence itself more than doubled. The optimization process found an optimal scale of approximately 0.49, which is much closer to the empirical standard deviation of the observations (~0.35).\n",
        "   \n",
        "2. In Bayesian terms, the evidence (marginal likelihood) quantifies how well a model predicts the observed data, integrating over all possible parameter values weighted by the prior. A higher evidence indicates that the model with the optimal scale is substantially better at explaining the observations Y=[0.5, 1.2]. This demonstrates that model comparison via evidence maximization can identify better-fitting models, with the optimal scale being closer to the true data-generating process."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
